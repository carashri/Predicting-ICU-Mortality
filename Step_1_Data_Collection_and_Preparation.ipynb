{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin by importing the libraries needed for data cleaning and preparation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Data was obtained from the MIMIC III database (https://mimic.physionet.org/).\n",
    "\n",
    "The first step is to read in the first dataset of interest, ICUSTAYS, and have a first look at the columns and rows. <br> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove patients who did not spend time in the ICU, since focus of this project is on ICU patients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61532, 12)\n"
     ]
    }
   ],
   "source": [
    "#read in ICUSTAYS dataset\n",
    "icu_stays = pd.read_csv('ICUSTAYS.csv.gz', compression='gzip' )\n",
    "print(icu_stays.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Columns of interest in the ICU dataset: \n",
    "- INTIME\n",
    "- HADM_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing intime: 0\n"
     ]
    }
   ],
   "source": [
    "#convert INTIME to datetime format. The errors='coerce' argument allows for missing values\n",
    "icu_stays.INTIME = pd.to_datetime(icu_stays.INTIME, format = '%Y-%m-%d %H:%M:%S', errors = 'coerce')\n",
    "\n",
    "# check to see if there are any null dates\n",
    "print('Number of missing intime:', icu_stays.INTIME.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>INTIME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>110404</td>\n",
       "      <td>2198-02-14 23:27:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>106296</td>\n",
       "      <td>2170-11-05 11:05:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>188028</td>\n",
       "      <td>2128-06-24 15:05:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>173727</td>\n",
       "      <td>2120-08-07 23:12:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>164716</td>\n",
       "      <td>2186-12-25 21:08:04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   HADM_ID              INTIME\n",
       "0   110404 2198-02-14 23:27:38\n",
       "1   106296 2170-11-05 11:05:29\n",
       "2   188028 2128-06-24 15:05:20\n",
       "3   173727 2120-08-07 23:12:42\n",
       "4   164716 2186-12-25 21:08:04"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reduce the columns to only 'HADM_ID' and 'INTIME'\n",
    "icu_stays = icu_stays[['HADM_ID', 'INTIME']]\n",
    "icu_stays.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>ICU</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>110404</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>106296</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>188028</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>173727</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>164716</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   HADM_ID   ICU\n",
       "0   110404  True\n",
       "1   106296  True\n",
       "2   188028  True\n",
       "3   173727  True\n",
       "4   164716  True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#add column ICU to indicate whether or not patient had ICU stay (all values will be true)\n",
    "icu_stays['ICU'] = True\n",
    "icu_stays = icu_stays[['HADM_ID', 'ICU']]\n",
    "icu_stays.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are  3746 duplicate hospital admissions.\n"
     ]
    }
   ],
   "source": [
    "#drop duplicate rows\n",
    "print('There are ', len(icu_stays[icu_stays.duplicated(['HADM_ID'])]), 'duplicate hospital admissions.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These admissions are patients who stayed more than once in the ICU on that admission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "icu_stays = icu_stays.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are now 0 duplicate hospital admissions.\n"
     ]
    }
   ],
   "source": [
    "print('There are now', len(icu_stays[icu_stays.duplicated(['HADM_ID'])]), 'duplicate hospital admissions.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 57786 entries, 0 to 61531\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype\n",
      "---  ------   --------------  -----\n",
      " 0   HADM_ID  57786 non-null  int64\n",
      " 1   ICU      57786 non-null  bool \n",
      "dtypes: bool(1), int64(1)\n",
      "memory usage: 959.3 KB\n"
     ]
    }
   ],
   "source": [
    "icu_stays.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(58976, 19)\n"
     ]
    }
   ],
   "source": [
    "admissions = pd.read_csv('ADMISSIONS.csv.gz', compression='gzip')\n",
    "print(admissions.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 58976 rows and 19 columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Columns of interest (or may be of interest) from the 'admissions' dataset include:\n",
    "- HOSPITAL_EXPIRE_FLAG\n",
    "- SUBJECT_ID\n",
    "- HADM_ID\n",
    "- ADMITTIME\n",
    "- DEATHTIME\n",
    "- ADMISSION_TYPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 58976 entries, 0 to 58975\n",
      "Data columns (total 19 columns):\n",
      " #   Column                Non-Null Count  Dtype \n",
      "---  ------                --------------  ----- \n",
      " 0   ROW_ID                58976 non-null  int64 \n",
      " 1   SUBJECT_ID            58976 non-null  int64 \n",
      " 2   HADM_ID               58976 non-null  int64 \n",
      " 3   ADMITTIME             58976 non-null  object\n",
      " 4   DISCHTIME             58976 non-null  object\n",
      " 5   DEATHTIME             5854 non-null   object\n",
      " 6   ADMISSION_TYPE        58976 non-null  object\n",
      " 7   ADMISSION_LOCATION    58976 non-null  object\n",
      " 8   DISCHARGE_LOCATION    58976 non-null  object\n",
      " 9   INSURANCE             58976 non-null  object\n",
      " 10  LANGUAGE              33644 non-null  object\n",
      " 11  RELIGION              58518 non-null  object\n",
      " 12  MARITAL_STATUS        48848 non-null  object\n",
      " 13  ETHNICITY             58976 non-null  object\n",
      " 14  EDREGTIME             30877 non-null  object\n",
      " 15  EDOUTTIME             30877 non-null  object\n",
      " 16  DIAGNOSIS             58951 non-null  object\n",
      " 17  HOSPITAL_EXPIRE_FLAG  58976 non-null  int64 \n",
      " 18  HAS_CHARTEVENTS_DATA  58976 non-null  int64 \n",
      "dtypes: int64(5), object(14)\n",
      "memory usage: 8.5+ MB\n"
     ]
    }
   ],
   "source": [
    "# exploring the data to determine the datatypes, in particular, of the date columns.\n",
    "admissions.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, convert the dates to datetime format for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing date admissions: 0\n",
      "Number of missing DEATHTIME: 53122\n",
      "Number of missing DISCHTIME: 0\n"
     ]
    }
   ],
   "source": [
    "#convert ADMITTIME and DEATHTIME to datetime format. The errors='coerce' argument allows for missing values\n",
    "admissions.ADMITTIME = pd.to_datetime(admissions.ADMITTIME, format = '%Y-%m-%d %H:%M:%S', errors = 'coerce')\n",
    "admissions.DEATHTIME = pd.to_datetime(admissions.DEATHTIME, format = '%Y-%m-%d %H:%M:%S', errors = 'coerce')\n",
    "admissions.DISCHTIME = pd.to_datetime(admissions.DISCHTIME, format = '%Y-%m-%d %H:%M:%S', errors = 'coerce')\n",
    "\n",
    "# check to see if there are any null dates\n",
    "print('Number of missing date admissions:', admissions.ADMITTIME.isnull().sum())\n",
    "print('Number of missing DEATHTIME:', admissions.DEATHTIME.isnull().sum())\n",
    "print('Number of missing DISCHTIME:', admissions.DISCHTIME.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now update the admissions dataset to include only HADM_ID that are in the icu_stays dataframe. Do this by completing a left merge on HADM_ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ROW_ID', 'SUBJECT_ID', 'HADM_ID', 'ADMITTIME', 'DISCHTIME',\n",
       "       'DEATHTIME', 'ADMISSION_TYPE', 'ADMISSION_LOCATION',\n",
       "       'DISCHARGE_LOCATION', 'INSURANCE', 'LANGUAGE', 'RELIGION',\n",
       "       'MARITAL_STATUS', 'ETHNICITY', 'EDREGTIME', 'EDOUTTIME', 'DIAGNOSIS',\n",
       "       'HOSPITAL_EXPIRE_FLAG', 'HAS_CHARTEVENTS_DATA'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "admissions.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "admissions = pd.merge(icu_stays[['HADM_ID']], admissions[['ROW_ID', 'SUBJECT_ID', 'HADM_ID', 'ADMITTIME', 'DISCHTIME',\n",
    "       'DEATHTIME', 'ADMISSION_TYPE', 'ADMISSION_LOCATION',\n",
    "       'DISCHARGE_LOCATION', 'INSURANCE', 'LANGUAGE', 'RELIGION',\n",
    "       'MARITAL_STATUS', 'ETHNICITY', 'EDREGTIME', 'EDOUTTIME', 'DIAGNOSIS',\n",
    "       'HOSPITAL_EXPIRE_FLAG', 'HAS_CHARTEVENTS_DATA']],\\\n",
    "            on=('HADM_ID') , how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 57786 entries, 0 to 57785\n",
      "Data columns (total 19 columns):\n",
      " #   Column                Non-Null Count  Dtype         \n",
      "---  ------                --------------  -----         \n",
      " 0   HADM_ID               57786 non-null  int64         \n",
      " 1   ROW_ID                57786 non-null  int64         \n",
      " 2   SUBJECT_ID            57786 non-null  int64         \n",
      " 3   ADMITTIME             57786 non-null  datetime64[ns]\n",
      " 4   DISCHTIME             57786 non-null  datetime64[ns]\n",
      " 5   DEATHTIME             5813 non-null   datetime64[ns]\n",
      " 6   ADMISSION_TYPE        57786 non-null  object        \n",
      " 7   ADMISSION_LOCATION    57786 non-null  object        \n",
      " 8   DISCHARGE_LOCATION    57786 non-null  object        \n",
      " 9   INSURANCE             57786 non-null  object        \n",
      " 10  LANGUAGE              32990 non-null  object        \n",
      " 11  RELIGION              57330 non-null  object        \n",
      " 12  MARITAL_STATUS        47796 non-null  object        \n",
      " 13  ETHNICITY             57786 non-null  object        \n",
      " 14  EDREGTIME             30638 non-null  object        \n",
      " 15  EDOUTTIME             30638 non-null  object        \n",
      " 16  DIAGNOSIS             57785 non-null  object        \n",
      " 17  HOSPITAL_EXPIRE_FLAG  57786 non-null  int64         \n",
      " 18  HAS_CHARTEVENTS_DATA  57786 non-null  int64         \n",
      "dtypes: datetime64[ns](3), int64(5), object(11)\n",
      "memory usage: 8.8+ MB\n"
     ]
    }
   ],
   "source": [
    "admissions.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next a new column ENDTIME will be created. This will indicate the when to end note collection for analysis - i.e., 24 hours after ADMITTIME or at time of death - whichever is sooner.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>ROW_ID</th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>ADMITTIME</th>\n",
       "      <th>DISCHTIME</th>\n",
       "      <th>DEATHTIME</th>\n",
       "      <th>ADMISSION_TYPE</th>\n",
       "      <th>ADMISSION_LOCATION</th>\n",
       "      <th>DISCHARGE_LOCATION</th>\n",
       "      <th>INSURANCE</th>\n",
       "      <th>...</th>\n",
       "      <th>RELIGION</th>\n",
       "      <th>MARITAL_STATUS</th>\n",
       "      <th>ETHNICITY</th>\n",
       "      <th>EDREGTIME</th>\n",
       "      <th>EDOUTTIME</th>\n",
       "      <th>DIAGNOSIS</th>\n",
       "      <th>HOSPITAL_EXPIRE_FLAG</th>\n",
       "      <th>HAS_CHARTEVENTS_DATA</th>\n",
       "      <th>ADMIT+24</th>\n",
       "      <th>ENDTIME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>110404</td>\n",
       "      <td>344</td>\n",
       "      <td>268</td>\n",
       "      <td>2198-02-11 13:40:00</td>\n",
       "      <td>2198-02-18 03:55:00</td>\n",
       "      <td>2198-02-18 03:55:00</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>EMERGENCY ROOM ADMIT</td>\n",
       "      <td>DEAD/EXPIRED</td>\n",
       "      <td>Medicare</td>\n",
       "      <td>...</td>\n",
       "      <td>CATHOLIC</td>\n",
       "      <td>SEPARATED</td>\n",
       "      <td>HISPANIC OR LATINO</td>\n",
       "      <td>2198-02-11 09:41:00</td>\n",
       "      <td>2198-02-11 15:18:00</td>\n",
       "      <td>DYSPNEA</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2198-02-12 13:40:00</td>\n",
       "      <td>2198-02-12 13:40:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>106296</td>\n",
       "      <td>345</td>\n",
       "      <td>269</td>\n",
       "      <td>2170-11-05 11:04:00</td>\n",
       "      <td>2170-11-27 18:00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>EMERGENCY ROOM ADMIT</td>\n",
       "      <td>HOME HEALTH CARE</td>\n",
       "      <td>Medicaid</td>\n",
       "      <td>...</td>\n",
       "      <td>UNOBTAINABLE</td>\n",
       "      <td>SINGLE</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>2170-11-05 07:22:00</td>\n",
       "      <td>2170-11-05 12:15:00</td>\n",
       "      <td>SEPSIS;PILONIDAL ABSCESS</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2170-11-06 11:04:00</td>\n",
       "      <td>2170-11-06 11:04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>188028</td>\n",
       "      <td>346</td>\n",
       "      <td>270</td>\n",
       "      <td>2128-06-23 18:26:00</td>\n",
       "      <td>2128-06-27 12:31:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>ELECTIVE</td>\n",
       "      <td>PHYS REFERRAL/NORMAL DELI</td>\n",
       "      <td>HOME HEALTH CARE</td>\n",
       "      <td>Medicare</td>\n",
       "      <td>...</td>\n",
       "      <td>JEHOVAH'S WITNESS</td>\n",
       "      <td>MARRIED</td>\n",
       "      <td>UNKNOWN/NOT SPECIFIED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CAROTID STENOSIS\\CAROTID ANGIOGRAM AND STENT</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2128-06-24 18:26:00</td>\n",
       "      <td>2128-06-24 18:26:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>173727</td>\n",
       "      <td>347</td>\n",
       "      <td>271</td>\n",
       "      <td>2120-08-07 18:56:00</td>\n",
       "      <td>2120-08-20 16:00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>TRANSFER FROM HOSP/EXTRAM</td>\n",
       "      <td>HOME</td>\n",
       "      <td>Private</td>\n",
       "      <td>...</td>\n",
       "      <td>NOT SPECIFIED</td>\n",
       "      <td>MARRIED</td>\n",
       "      <td>PATIENT DECLINED TO ANSWER</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GALLSTONE PANCREATITIS</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2120-08-08 18:56:00</td>\n",
       "      <td>2120-08-08 18:56:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>164716</td>\n",
       "      <td>348</td>\n",
       "      <td>272</td>\n",
       "      <td>2186-12-25 21:06:00</td>\n",
       "      <td>2187-01-02 14:57:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>TRANSFER FROM HOSP/EXTRAM</td>\n",
       "      <td>HOME</td>\n",
       "      <td>Medicare</td>\n",
       "      <td>...</td>\n",
       "      <td>UNOBTAINABLE</td>\n",
       "      <td>MARRIED</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PULMONARY EMBOLIS</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2186-12-26 21:06:00</td>\n",
       "      <td>2186-12-26 21:06:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   HADM_ID  ROW_ID  SUBJECT_ID           ADMITTIME           DISCHTIME  \\\n",
       "0   110404     344         268 2198-02-11 13:40:00 2198-02-18 03:55:00   \n",
       "1   106296     345         269 2170-11-05 11:04:00 2170-11-27 18:00:00   \n",
       "2   188028     346         270 2128-06-23 18:26:00 2128-06-27 12:31:00   \n",
       "3   173727     347         271 2120-08-07 18:56:00 2120-08-20 16:00:00   \n",
       "4   164716     348         272 2186-12-25 21:06:00 2187-01-02 14:57:00   \n",
       "\n",
       "            DEATHTIME ADMISSION_TYPE         ADMISSION_LOCATION  \\\n",
       "0 2198-02-18 03:55:00      EMERGENCY       EMERGENCY ROOM ADMIT   \n",
       "1                 NaT      EMERGENCY       EMERGENCY ROOM ADMIT   \n",
       "2                 NaT       ELECTIVE  PHYS REFERRAL/NORMAL DELI   \n",
       "3                 NaT      EMERGENCY  TRANSFER FROM HOSP/EXTRAM   \n",
       "4                 NaT      EMERGENCY  TRANSFER FROM HOSP/EXTRAM   \n",
       "\n",
       "  DISCHARGE_LOCATION INSURANCE  ...           RELIGION MARITAL_STATUS  \\\n",
       "0       DEAD/EXPIRED  Medicare  ...           CATHOLIC      SEPARATED   \n",
       "1   HOME HEALTH CARE  Medicaid  ...       UNOBTAINABLE         SINGLE   \n",
       "2   HOME HEALTH CARE  Medicare  ...  JEHOVAH'S WITNESS        MARRIED   \n",
       "3               HOME   Private  ...      NOT SPECIFIED        MARRIED   \n",
       "4               HOME  Medicare  ...       UNOBTAINABLE        MARRIED   \n",
       "\n",
       "                    ETHNICITY            EDREGTIME            EDOUTTIME  \\\n",
       "0          HISPANIC OR LATINO  2198-02-11 09:41:00  2198-02-11 15:18:00   \n",
       "1                       WHITE  2170-11-05 07:22:00  2170-11-05 12:15:00   \n",
       "2       UNKNOWN/NOT SPECIFIED                  NaN                  NaN   \n",
       "3  PATIENT DECLINED TO ANSWER                  NaN                  NaN   \n",
       "4                       WHITE                  NaN                  NaN   \n",
       "\n",
       "                                      DIAGNOSIS HOSPITAL_EXPIRE_FLAG  \\\n",
       "0                                       DYSPNEA                    1   \n",
       "1                      SEPSIS;PILONIDAL ABSCESS                    0   \n",
       "2  CAROTID STENOSIS\\CAROTID ANGIOGRAM AND STENT                    0   \n",
       "3                        GALLSTONE PANCREATITIS                    0   \n",
       "4                             PULMONARY EMBOLIS                    0   \n",
       "\n",
       "   HAS_CHARTEVENTS_DATA            ADMIT+24             ENDTIME  \n",
       "0                     1 2198-02-12 13:40:00 2198-02-12 13:40:00  \n",
       "1                     1 2170-11-06 11:04:00 2170-11-06 11:04:00  \n",
       "2                     1 2128-06-24 18:26:00 2128-06-24 18:26:00  \n",
       "3                     1 2120-08-08 18:56:00 2120-08-08 18:56:00  \n",
       "4                     1 2186-12-26 21:06:00 2186-12-26 21:06:00  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "admissions['ADMIT+24'] = admissions['ADMITTIME'] + pd.DateOffset(days=1)\n",
    "admissions['ENDTIME'] = admissions['ADMIT+24']\n",
    "\n",
    "admissions.ENDTIME = np.where(admissions['DEATHTIME'] < admissions['ENDTIME'], admissions['DEATHTIME']+pd.DateOffset(hours=-2), admissions['ENDTIME'])\n",
    "admissions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 57786 entries, 0 to 57785\n",
      "Data columns (total 21 columns):\n",
      " #   Column                Non-Null Count  Dtype         \n",
      "---  ------                --------------  -----         \n",
      " 0   HADM_ID               57786 non-null  int64         \n",
      " 1   ROW_ID                57786 non-null  int64         \n",
      " 2   SUBJECT_ID            57786 non-null  int64         \n",
      " 3   ADMITTIME             57786 non-null  datetime64[ns]\n",
      " 4   DISCHTIME             57786 non-null  datetime64[ns]\n",
      " 5   DEATHTIME             5813 non-null   datetime64[ns]\n",
      " 6   ADMISSION_TYPE        57786 non-null  object        \n",
      " 7   ADMISSION_LOCATION    57786 non-null  object        \n",
      " 8   DISCHARGE_LOCATION    57786 non-null  object        \n",
      " 9   INSURANCE             57786 non-null  object        \n",
      " 10  LANGUAGE              32990 non-null  object        \n",
      " 11  RELIGION              57330 non-null  object        \n",
      " 12  MARITAL_STATUS        47796 non-null  object        \n",
      " 13  ETHNICITY             57786 non-null  object        \n",
      " 14  EDREGTIME             30638 non-null  object        \n",
      " 15  EDOUTTIME             30638 non-null  object        \n",
      " 16  DIAGNOSIS             57785 non-null  object        \n",
      " 17  HOSPITAL_EXPIRE_FLAG  57786 non-null  int64         \n",
      " 18  HAS_CHARTEVENTS_DATA  57786 non-null  int64         \n",
      " 19  ADMIT+24              57786 non-null  datetime64[ns]\n",
      " 20  ENDTIME               57786 non-null  datetime64[ns]\n",
      "dtypes: datetime64[ns](5), int64(5), object(11)\n",
      "memory usage: 9.7+ MB\n"
     ]
    }
   ],
   "source": [
    "admissions.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twenty-four hours of notes need to be available in order to compare samples equally. Therefore, patients with a length of stay 24 hours or less are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>ROW_ID</th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>ADMITTIME</th>\n",
       "      <th>DISCHTIME</th>\n",
       "      <th>DEATHTIME</th>\n",
       "      <th>ADMISSION_TYPE</th>\n",
       "      <th>ADMISSION_LOCATION</th>\n",
       "      <th>DISCHARGE_LOCATION</th>\n",
       "      <th>INSURANCE</th>\n",
       "      <th>...</th>\n",
       "      <th>MARITAL_STATUS</th>\n",
       "      <th>ETHNICITY</th>\n",
       "      <th>EDREGTIME</th>\n",
       "      <th>EDOUTTIME</th>\n",
       "      <th>DIAGNOSIS</th>\n",
       "      <th>HOSPITAL_EXPIRE_FLAG</th>\n",
       "      <th>HAS_CHARTEVENTS_DATA</th>\n",
       "      <th>ADMIT+24</th>\n",
       "      <th>ENDTIME</th>\n",
       "      <th>LOS_HOSP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>110404</td>\n",
       "      <td>344</td>\n",
       "      <td>268</td>\n",
       "      <td>2198-02-11 13:40:00</td>\n",
       "      <td>2198-02-18 03:55:00</td>\n",
       "      <td>2198-02-18 03:55:00</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>EMERGENCY ROOM ADMIT</td>\n",
       "      <td>DEAD/EXPIRED</td>\n",
       "      <td>Medicare</td>\n",
       "      <td>...</td>\n",
       "      <td>SEPARATED</td>\n",
       "      <td>HISPANIC OR LATINO</td>\n",
       "      <td>2198-02-11 09:41:00</td>\n",
       "      <td>2198-02-11 15:18:00</td>\n",
       "      <td>DYSPNEA</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2198-02-12 13:40:00</td>\n",
       "      <td>2198-02-12 13:40:00</td>\n",
       "      <td>6 days 14:15:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   HADM_ID  ROW_ID  SUBJECT_ID           ADMITTIME           DISCHTIME  \\\n",
       "0   110404     344         268 2198-02-11 13:40:00 2198-02-18 03:55:00   \n",
       "\n",
       "            DEATHTIME ADMISSION_TYPE    ADMISSION_LOCATION DISCHARGE_LOCATION  \\\n",
       "0 2198-02-18 03:55:00      EMERGENCY  EMERGENCY ROOM ADMIT       DEAD/EXPIRED   \n",
       "\n",
       "  INSURANCE  ... MARITAL_STATUS           ETHNICITY            EDREGTIME  \\\n",
       "0  Medicare  ...      SEPARATED  HISPANIC OR LATINO  2198-02-11 09:41:00   \n",
       "\n",
       "             EDOUTTIME DIAGNOSIS HOSPITAL_EXPIRE_FLAG HAS_CHARTEVENTS_DATA  \\\n",
       "0  2198-02-11 15:18:00   DYSPNEA                    1                    1   \n",
       "\n",
       "             ADMIT+24             ENDTIME        LOS_HOSP  \n",
       "0 2198-02-12 13:40:00 2198-02-12 13:40:00 6 days 14:15:00  \n",
       "\n",
       "[1 rows x 22 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first create a column that calculated length of stay\n",
    "admissions['LOS_HOSP'] = admissions['DISCHTIME'] - admissions['ADMITTIME']\n",
    "admissions.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of hospitalizations more than 24 hours:  55727\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(55727, 22)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Number of hospitalizations more than 24 hours: ', len(admissions.loc[admissions['LOS_HOSP']>'24 hours']))\n",
    "admissions = admissions.loc[admissions['LOS_HOSP']>'1 day']\n",
    "admissions.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The next step is to add the target variable 'DEATH' which indicates whether or not a hospitalization resulted in death.\n",
    "# positive death=1/negative death=0\n",
    "\n",
    "#admissions['DEATH'] = admissions['DEATHTIME']>pd.Timestamp('00:00:00')\n",
    "#admissions.DEATH.value_counts()\n",
    "#print(admissions.DEATH.value_counts())\n",
    "#print(admissions.DEATH.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I later realized that the column \"HOSPITAL_EXPIRE_FLAG\" is same as the \"DEATH\" column that was created.\n",
    "# drop DEATH column\n",
    "#admissions.drop('DEATH', axis=1, inplace=True)\n",
    "#admissions.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55727, 22)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "admissions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    50856\n",
       "1     4871\n",
       "Name: HOSPITAL_EXPIRE_FLAG, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "admissions.HOSPITAL_EXPIRE_FLAG.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4871 out of 55727 hospital admissions resulted in death (8.7%). As should be expected, this dataset is imbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to examine the caregiver notes. <br>\n",
    "<br>\n",
    "### Next: Read in the caregiver notes dataset and get an overview of its rows and features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caras\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3071: DtypeWarning: Columns (4,5) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROW_ID</th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>CHARTDATE</th>\n",
       "      <th>CHARTTIME</th>\n",
       "      <th>STORETIME</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "      <th>CGID</th>\n",
       "      <th>ISERROR</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>174</td>\n",
       "      <td>22532</td>\n",
       "      <td>167853.0</td>\n",
       "      <td>2151-08-04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Admission Date:  [**2151-7-16**]       Dischar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>175</td>\n",
       "      <td>13702</td>\n",
       "      <td>107527.0</td>\n",
       "      <td>2118-06-14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Admission Date:  [**2118-6-2**]       Discharg...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ROW_ID  SUBJECT_ID   HADM_ID   CHARTDATE CHARTTIME STORETIME  \\\n",
       "0     174       22532  167853.0  2151-08-04       NaN       NaN   \n",
       "1     175       13702  107527.0  2118-06-14       NaN       NaN   \n",
       "\n",
       "            CATEGORY DESCRIPTION  CGID  ISERROR  \\\n",
       "0  Discharge summary      Report   NaN      NaN   \n",
       "1  Discharge summary      Report   NaN      NaN   \n",
       "\n",
       "                                                TEXT  \n",
       "0  Admission Date:  [**2151-7-16**]       Dischar...  \n",
       "1  Admission Date:  [**2118-6-2**]       Discharg...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read in the caregiver notes dataset.\n",
    "\n",
    "notes = pd.read_csv('NOTEEVENTS.csv.gz', compression='gzip')\n",
    "notes.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2083180 entries, 0 to 2083179\n",
      "Data columns (total 11 columns):\n",
      " #   Column       Dtype  \n",
      "---  ------       -----  \n",
      " 0   ROW_ID       int64  \n",
      " 1   SUBJECT_ID   int64  \n",
      " 2   HADM_ID      float64\n",
      " 3   CHARTDATE    object \n",
      " 4   CHARTTIME    object \n",
      " 5   STORETIME    object \n",
      " 6   CATEGORY     object \n",
      " 7   DESCRIPTION  object \n",
      " 8   CGID         float64\n",
      " 9   ISERROR      float64\n",
      " 10  TEXT         object \n",
      "dtypes: float64(3), int64(2), object(6)\n",
      "memory usage: 174.8+ MB\n"
     ]
    }
   ],
   "source": [
    "notes.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ISERROR\n",
    "A ‘1’ in the ISERROR column indicates that a physician has identified this note as an error. These rows will be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    886\n",
       "Name: ISERROR, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notes.ISERROR.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2082294, 11)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notes = notes.loc[notes['ISERROR']!=1]\n",
    "notes.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns of interest from the NOTES dataset:\n",
    "- SUBJECT_ID\n",
    "- HADM_ID\n",
    "- CHARTDATE and CHARTTIME\n",
    "- CATEGORY\n",
    "- TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Nursing/other        822497\n",
       "Radiology            522279\n",
       "Nursing              223182\n",
       "ECG                  209051\n",
       "Physician            141281\n",
       "Discharge summary     59652\n",
       "Echo                  45794\n",
       "Respiratory           31701\n",
       "Nutrition              9400\n",
       "General                8236\n",
       "Rehab Services         5408\n",
       "Social Work            2661\n",
       "Case Management         953\n",
       "Pharmacy                101\n",
       "Consult                  98\n",
       "Name: CATEGORY, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Viewing categories of caregiver notes to better understand the dataset.\n",
    "notes.CATEGORY.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2082294 entries, 0 to 2083179\n",
      "Data columns (total 11 columns):\n",
      " #   Column       Dtype  \n",
      "---  ------       -----  \n",
      " 0   ROW_ID       int64  \n",
      " 1   SUBJECT_ID   int64  \n",
      " 2   HADM_ID      float64\n",
      " 3   CHARTDATE    object \n",
      " 4   CHARTTIME    object \n",
      " 5   STORETIME    object \n",
      " 6   CATEGORY     object \n",
      " 7   DESCRIPTION  object \n",
      " 8   CGID         float64\n",
      " 9   ISERROR      float64\n",
      " 10  TEXT         object \n",
      "dtypes: float64(3), int64(2), object(6)\n",
      "memory usage: 190.6+ MB\n"
     ]
    }
   ],
   "source": [
    "notes.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHARTDATE records the date at which the note was charted. CHARTDATE will always have a time value of 00:00:00.\n",
    "\n",
    "CHARTTIME records the date and time at which the note was charted. If both CHARTDATE and CHARTTIME exist, then the date portions will be identical. All records have a CHARTDATE. A subset are missing CHARTTIME. More specifically, notes with a CATEGORY value of ‘Discharge Summary’, ‘ECG’, and ‘Echo’ never have a CHARTTIME, only CHARTDATE. Other categories almost always have both CHARTTIME and CHARTDATE, but there is a small amount of missing data for CHARTTIME (usually less than 0.5% of the total number of notes for that category)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing CHARTTIME: 316566\n",
      "Number of missing CHARTDATE: 0\n"
     ]
    }
   ],
   "source": [
    "#convert the chart dates and times into datetime format\n",
    "\n",
    "notes.CHARTTIME = pd.to_datetime(notes.CHARTTIME, format = '%Y-%m-%d %H:%M:%S', errors = 'coerce')\n",
    "notes.CHARTDATE = pd.to_datetime(notes.CHARTDATE, format = '%Y-%m-%d %H:%M:%S', errors = 'coerce')\n",
    "\n",
    "\n",
    "# check to see if there are any null dates\n",
    "print('Number of missing CHARTTIME:', notes.CHARTTIME.isnull().sum())\n",
    "print('Number of missing CHARTDATE:', notes.CHARTDATE.isnull().sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are so many missing CHARTTIME values, CHARTDATE will be used for processing and analysis (below). Before analyzing further, the dataframes will be merged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['HADM_ID', 'ROW_ID', 'SUBJECT_ID', 'ADMITTIME', 'DISCHTIME',\n",
       "       'DEATHTIME', 'ADMISSION_TYPE', 'ADMISSION_LOCATION',\n",
       "       'DISCHARGE_LOCATION', 'INSURANCE', 'LANGUAGE', 'RELIGION',\n",
       "       'MARITAL_STATUS', 'ETHNICITY', 'EDREGTIME', 'EDOUTTIME', 'DIAGNOSIS',\n",
       "       'HOSPITAL_EXPIRE_FLAG', 'HAS_CHARTEVENTS_DATA', 'ADMIT+24', 'ENDTIME',\n",
       "       'LOS_HOSP'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "admissions.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are  1830890 rows and  12 columns.\n"
     ]
    }
   ],
   "source": [
    "# Merging the 'admissions' and 'notes' together. A left merge is used so that all rows for hospital admissions are included\n",
    "# and any caregiver notes that are not associated with a hospital admission are dropped.\n",
    "df=pd.merge(admissions[['SUBJECT_ID', 'HADM_ID', 'LOS_HOSP','ADMITTIME', 'HOSPITAL_EXPIRE_FLAG', 'ADMISSION_TYPE', 'DEATHTIME','ADMIT+24', 'ENDTIME']],\\\n",
    "            notes[['SUBJECT_ID', 'HADM_ID', 'CHARTDATE', 'CATEGORY','TEXT']], \\\n",
    "            on=('HADM_ID', 'SUBJECT_ID') , how='left', suffixes=('adm','note'))\n",
    "print('There are ', len(df), 'rows and ',len(df.columns), 'columns.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SUBJECT_ID',\n",
       " 'HADM_ID',\n",
       " 'LOS_HOSP',\n",
       " 'ADMITTIME',\n",
       " 'HOSPITAL_EXPIRE_FLAG',\n",
       " 'ADMISSION_TYPE',\n",
       " 'DEATHTIME',\n",
       " 'ADMIT+24',\n",
       " 'ENDTIME',\n",
       " 'CHARTDATE',\n",
       " 'CATEGORY',\n",
       " 'TEXT']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next update the dataframe to only include notes taken within 24 hours of admission, or up until 2 hours before time of death if patient expired in the first 24 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = df[df['CHARTDATE'] <= df['ENDTIME']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(428039, 12)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to combine all the text samples into one single TEXT row per admission, a new dataframe, 'text' is created. Next it will be merged back with the original dataframe with  CHARTTIME and CATEGORY columns dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df[['HADM_ID', 'TEXT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   HADM_ID                                               TEXT\n",
      "0   100001  [**2117-9-11**] 11:12 AM\\n CHEST (PA & LAT)   ...\n",
      "1   100003  PATIENT/TEST INFORMATION:\\nIndication: Left ve...\n",
      "2   100006  Sinus tachycardia\\nLeft axis deviation - anter...\n",
      "3   100007  Sinus rhythm\\nAtrial premature complex\\nConsid...\n",
      "4   100009  PATIENT/TEST INFORMATION:\\nIndication: Abnorma...\n"
     ]
    }
   ],
   "source": [
    "grouped_HADM = text.groupby(\"HADM_ID\")\n",
    "\n",
    "grouped_text = grouped_HADM[\"TEXT\"].agg(lambda column: \"\".join(column))\n",
    "\n",
    "grouped_text = grouped_text.reset_index(name=\"TEXT\")\n",
    "\n",
    "print(grouped_text.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53977, 2)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['SUBJECT_ID', 'HADM_ID', 'LOS_HOSP', 'ADMITTIME',\n",
       "       'HOSPITAL_EXPIRE_FLAG', 'ADMISSION_TYPE', 'DEATHTIME', 'ADMIT+24',\n",
       "       'ENDTIME', 'CHARTDATE', 'CATEGORY', 'TEXT'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are  428039 rows and  10 columns.\n"
     ]
    }
   ],
   "source": [
    "# Merging the 'grouped_text' and 'df' together. A left merge is used so that all rows for hospital admissions are included\n",
    "# and any caregiver notes that are not associated with a hospital admission are dropped.\n",
    "df2=pd.merge(grouped_text[['HADM_ID', 'TEXT']], df[['SUBJECT_ID', 'HADM_ID','LOS_HOSP', 'ADMITTIME', 'HOSPITAL_EXPIRE_FLAG',\\\n",
    "                                                    'ADMISSION_TYPE', 'DEATHTIME', 'ADMIT+24','ENDTIME']],\\\n",
    "             on=('HADM_ID') , how='left')\n",
    "print('There are ', len(df2), 'rows and ',len(df2.columns), 'columns.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are now  53977 rows and  10 columns in df2.\n"
     ]
    }
   ],
   "source": [
    "print('There are now ', len(df2), 'rows and ', len(df2.columns), 'columns in df2.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>LOS_HOSP</th>\n",
       "      <th>ADMITTIME</th>\n",
       "      <th>HOSPITAL_EXPIRE_FLAG</th>\n",
       "      <th>ADMISSION_TYPE</th>\n",
       "      <th>DEATHTIME</th>\n",
       "      <th>ADMIT+24</th>\n",
       "      <th>ENDTIME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>72966</th>\n",
       "      <td>117100</td>\n",
       "      <td>[**2119-6-8**] 9:52 PM\\n CHEST (PORTABLE AP)  ...</td>\n",
       "      <td>8697</td>\n",
       "      <td>2 days 22:51:00</td>\n",
       "      <td>2119-06-08 15:23:00</td>\n",
       "      <td>1</td>\n",
       "      <td>URGENT</td>\n",
       "      <td>2119-06-08 14:14:00</td>\n",
       "      <td>2119-06-09 15:23:00</td>\n",
       "      <td>2119-06-08 12:14:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       HADM_ID                                               TEXT  SUBJECT_ID  \\\n",
       "72966   117100  [**2119-6-8**] 9:52 PM\\n CHEST (PORTABLE AP)  ...        8697   \n",
       "\n",
       "             LOS_HOSP           ADMITTIME  HOSPITAL_EXPIRE_FLAG  \\\n",
       "72966 2 days 22:51:00 2119-06-08 15:23:00                     1   \n",
       "\n",
       "      ADMISSION_TYPE           DEATHTIME            ADMIT+24  \\\n",
       "72966         URGENT 2119-06-08 14:14:00 2119-06-09 15:23:00   \n",
       "\n",
       "                  ENDTIME  \n",
       "72966 2119-06-08 12:14:00  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check if any samples died before admission and remove them from the dataframe. Save as new dataframe.\n",
    "\n",
    "df2.loc[df2['DEATHTIME'] < df2['ADMITTIME']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53976, 10)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df2.drop(df2[df2['HADM_ID'] == 117100].index)\n",
    "df2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is now ready to begin processing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the merged dataframe.\n",
    "df2.to_csv('df2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For use in savings calculations (later), calculate the average length of hospital stay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average hospitalization length for patients who stayed in the ICU was  10 days 13:56:24.582697\n"
     ]
    }
   ],
   "source": [
    "LOS_hosp_mean = admissions.LOS_HOSP.mean()\n",
    "print('The average hospitalization length for patients who stayed in the ICU was ', LOS_hosp_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To also use in savings analysis, calculate the average number of days in the ICU per admission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in ICU stays data so that we can determine average number of days in ICU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61532, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROW_ID</th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>ICUSTAY_ID</th>\n",
       "      <th>DBSOURCE</th>\n",
       "      <th>FIRST_CAREUNIT</th>\n",
       "      <th>LAST_CAREUNIT</th>\n",
       "      <th>FIRST_WARDID</th>\n",
       "      <th>LAST_WARDID</th>\n",
       "      <th>INTIME</th>\n",
       "      <th>OUTTIME</th>\n",
       "      <th>LOS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>365</td>\n",
       "      <td>268</td>\n",
       "      <td>110404</td>\n",
       "      <td>280836</td>\n",
       "      <td>carevue</td>\n",
       "      <td>MICU</td>\n",
       "      <td>MICU</td>\n",
       "      <td>52</td>\n",
       "      <td>52</td>\n",
       "      <td>2198-02-14 23:27:38</td>\n",
       "      <td>2198-02-18 05:26:11</td>\n",
       "      <td>3.2490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>366</td>\n",
       "      <td>269</td>\n",
       "      <td>106296</td>\n",
       "      <td>206613</td>\n",
       "      <td>carevue</td>\n",
       "      <td>MICU</td>\n",
       "      <td>MICU</td>\n",
       "      <td>52</td>\n",
       "      <td>52</td>\n",
       "      <td>2170-11-05 11:05:29</td>\n",
       "      <td>2170-11-08 17:46:57</td>\n",
       "      <td>3.2788</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ROW_ID  SUBJECT_ID  HADM_ID  ICUSTAY_ID DBSOURCE FIRST_CAREUNIT  \\\n",
       "0     365         268   110404      280836  carevue           MICU   \n",
       "1     366         269   106296      206613  carevue           MICU   \n",
       "\n",
       "  LAST_CAREUNIT  FIRST_WARDID  LAST_WARDID               INTIME  \\\n",
       "0          MICU            52           52  2198-02-14 23:27:38   \n",
       "1          MICU            52           52  2170-11-05 11:05:29   \n",
       "\n",
       "               OUTTIME     LOS  \n",
       "0  2198-02-18 05:26:11  3.2490  \n",
       "1  2170-11-08 17:46:57  3.2788  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "icu_stays = pd.read_csv('ICUSTAYS.csv.gz', compression='gzip' )\n",
    "print(icu_stays.shape)\n",
    "icu_stays.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Columns of interest in the ICU dataset: \n",
    "- INTIME\n",
    "- OUTTIME\n",
    "- LOS (length of stay)\n",
    "- HADM_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing outtime: 10\n",
      "Number of missing intime: 0\n"
     ]
    }
   ],
   "source": [
    "#convert OUTTIME, and INTIME to datetime format. The errors='coerce' argument allows for missing values\n",
    "icu_stays.OUTTIME = pd.to_datetime(icu_stays.OUTTIME, format = '%Y-%m-%d %H:%M:%S', errors = 'coerce')\n",
    "icu_stays.INTIME = pd.to_datetime(icu_stays.INTIME, format = '%Y-%m-%d %H:%M:%S', errors = 'coerce')\n",
    "\n",
    "# check to see if there are any null dates\n",
    "print('Number of missing outtime:', icu_stays.OUTTIME.isnull().sum())\n",
    "print('Number of missing intime:', icu_stays.INTIME.isnull().sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the ICU stays with missing outtimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing outtime: 0\n"
     ]
    }
   ],
   "source": [
    "icu_stays = icu_stays.dropna(subset=['OUTTIME'])\n",
    "print('Number of missing outtime:', icu_stays.OUTTIME.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 61522 entries, 0 to 61531\n",
      "Data columns (total 12 columns):\n",
      " #   Column          Non-Null Count  Dtype         \n",
      "---  ------          --------------  -----         \n",
      " 0   ROW_ID          61522 non-null  int64         \n",
      " 1   SUBJECT_ID      61522 non-null  int64         \n",
      " 2   HADM_ID         61522 non-null  int64         \n",
      " 3   ICUSTAY_ID      61522 non-null  int64         \n",
      " 4   DBSOURCE        61522 non-null  object        \n",
      " 5   FIRST_CAREUNIT  61522 non-null  object        \n",
      " 6   LAST_CAREUNIT   61522 non-null  object        \n",
      " 7   FIRST_WARDID    61522 non-null  int64         \n",
      " 8   LAST_WARDID     61522 non-null  int64         \n",
      " 9   INTIME          61522 non-null  datetime64[ns]\n",
      " 10  OUTTIME         61522 non-null  datetime64[ns]\n",
      " 11  LOS             61522 non-null  float64       \n",
      "dtypes: datetime64[ns](2), float64(1), int64(6), object(3)\n",
      "memory usage: 6.1+ MB\n"
     ]
    }
   ],
   "source": [
    "icu_stays.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average length of ICU stay:  4.92 days\n"
     ]
    }
   ],
   "source": [
    "ICU_LOS_mean = round(icu_stays.LOS.mean(), 2)\n",
    "print('average length of ICU stay: ', ICU_LOS_mean, 'days')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>LOS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>110404</td>\n",
       "      <td>3.2490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>106296</td>\n",
       "      <td>3.2788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>188028</td>\n",
       "      <td>2.8939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>173727</td>\n",
       "      <td>2.0600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>164716</td>\n",
       "      <td>1.6202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61527</th>\n",
       "      <td>143774</td>\n",
       "      <td>2.1894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61528</th>\n",
       "      <td>123750</td>\n",
       "      <td>2.4942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61529</th>\n",
       "      <td>196881</td>\n",
       "      <td>0.9259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61530</th>\n",
       "      <td>118475</td>\n",
       "      <td>2.3346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61531</th>\n",
       "      <td>156386</td>\n",
       "      <td>4.5022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61522 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       HADM_ID     LOS\n",
       "0       110404  3.2490\n",
       "1       106296  3.2788\n",
       "2       188028  2.8939\n",
       "3       173727  2.0600\n",
       "4       164716  1.6202\n",
       "...        ...     ...\n",
       "61527   143774  2.1894\n",
       "61528   123750  2.4942\n",
       "61529   196881  0.9259\n",
       "61530   118475  2.3346\n",
       "61531   156386  4.5022\n",
       "\n",
       "[61522 rows x 2 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_los = icu_stays[['HADM_ID', 'LOS']]\n",
    "df_los"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>LOS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>110404</td>\n",
       "      <td>3.2490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>106296</td>\n",
       "      <td>3.2788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>188028</td>\n",
       "      <td>2.8939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>173727</td>\n",
       "      <td>2.0600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>164716</td>\n",
       "      <td>1.6202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61527</th>\n",
       "      <td>143774</td>\n",
       "      <td>2.1894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61528</th>\n",
       "      <td>123750</td>\n",
       "      <td>2.4942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61529</th>\n",
       "      <td>196881</td>\n",
       "      <td>0.9259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61530</th>\n",
       "      <td>118475</td>\n",
       "      <td>2.3346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61531</th>\n",
       "      <td>156386</td>\n",
       "      <td>4.5022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61522 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       HADM_ID     LOS\n",
       "0       110404  3.2490\n",
       "1       106296  3.2788\n",
       "2       188028  2.8939\n",
       "3       173727  2.0600\n",
       "4       164716  1.6202\n",
       "...        ...     ...\n",
       "61527   143774  2.1894\n",
       "61528   123750  2.4942\n",
       "61529   196881  0.9259\n",
       "61530   118475  2.3346\n",
       "61531   156386  4.5022\n",
       "\n",
       "[61522 rows x 2 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_los"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are  3746 duplicate hospital admissions.\n"
     ]
    }
   ],
   "source": [
    "print('There are ', len(df_los[df_los.duplicated(['HADM_ID'])]), 'duplicate hospital admissions.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These admissions are patients who stayed more than once in the ICU on that admission. LOS_ICU values for these patients need to be combined for a total LOS_ICU for each admission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>LOS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [HADM_ID, LOS]\n",
       "Index: []"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_los.loc[df_los['LOS'] < 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add column, LOS_ICU_TOTAL \n",
    "#df['LOS_ICU'] = df['LOS_ICU'].dt.days\n",
    "#df.LOS_ICU\n",
    "\n",
    "#df.LOS_ICU_TOTAL = np.where(df[df.duplicated(['HADM_ID'])], df['LOS_ICU'].sum(), df['LOS_ICU'])\n",
    "\n",
    "#df['is_dup'] = df[['lat', 'lon']].duplicated()\n",
    "#df['dups'] = df.groupby(['lat','lon']).is_dup.transform(np.sum)\n",
    "# df outputs:\n",
    "\n",
    "df_los = df_los.groupby(['HADM_ID'], as_index=False)['LOS'].sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 57776 entries, 0 to 57775\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   HADM_ID  57776 non-null  int64  \n",
      " 1   LOS      57776 non-null  float64\n",
      "dtypes: float64(1), int64(1)\n",
      "memory usage: 1.3 MB\n"
     ]
    }
   ],
   "source": [
    "df_los.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are now 0 duplicate hospital admissions.\n"
     ]
    }
   ],
   "source": [
    "print('There are now', len(df_los[df_los.duplicated(['HADM_ID'])]), 'duplicate hospital admissions.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average length of ICU stay was  5.24  days.\n"
     ]
    }
   ],
   "source": [
    "ICU_LOS_mean_updated = round(df_los['LOS'].mean(), 2)\n",
    "\n",
    "print('The average length of ICU stay was ', ICU_LOS_mean_updated, ' days.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-958e11fdcbb0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdf2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'df2.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    674\u001b[0m         )\n\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    453\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 454\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    455\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m         \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1131\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"nrows\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1133\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m         \u001b[1;31m# May alter columns / col_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   2035\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2036\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2037\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2038\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2039\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\dtypes\\common.py\u001b[0m in \u001b[0;36mis_categorical_dtype\u001b[1;34m(arr_or_dtype)\u001b[0m\n\u001b[0;32m    570\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0marr_or_dtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mCategoricalDtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df2 = pd.read_csv('df2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#confirming data types and that there are no null values\n",
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reorder the columns and remove those no longer needed.\n",
    "df3 = df2[['HADM_ID', 'SUBJECT_ID', 'LOS_HOSP', 'HOSPITAL_EXPIRE_FLAG', 'TEXT']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step before beginning data analysis is to split the data into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing data further for ML model fitting, separating target variable from features\n",
    "\n",
    "X = df3.drop(['HOSPITAL_EXPIRE_FLAG'], axis='columns') #feature columns\n",
    "y = df3.HOSPITAL_EXPIRE_FLAG #target variable\n",
    "\n",
    "# Split the data into train and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, shuffle=True, random_state=42, stratify=y)\n",
    "\n",
    "print('number of negative training samples: ', len(y_train[y_train==0]))\n",
    "print('total number of negative training samples: ', len(y_train))\n",
    "print('total number of test samples: ', len(y_test))\n",
    "print('number of negative test samples: ', len(y_test[y_test==0]))\n",
    "\n",
    "#Confirming target variable counts\n",
    "y.value_counts()\n",
    "print(len(y[y==1]), 'out of ', len(y), ' patients (', round(len(y[y==1])*100/len(y),1), '%) expired while in the ICU.') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle dataset in order to randomize all of the samples\n",
    "df4 = df3.sample(n = len(df3), random_state = 42)\n",
    "df4 = df4.reset_index(drop = True) #resetting index for the newly shuffled dataset\n",
    "\n",
    "\n",
    "df_valid_test=df4.sample(frac=0.30,random_state=42) # Save 30% of the data as validation and testing data.\n",
    "df_test = df_valid_test.sample(frac = 0.5, random_state = 42) #Of the validation & testing data, 50% is set aside for testing \n",
    "df_valid = df_valid_test.drop(df_test.index) #dropping the index for the validation data\n",
    "\n",
    "df_train_all=df4.drop(df_valid_test.index) # remove the rows used for validation; We are left with the rest of the data which will be used for training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, working with the training data, split the data into 2 categories based on mortality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# split the training data into positive (death) and negative (no death)\n",
    "positive = df_train_all.HOSPITAL_EXPIRE_FLAG == 1\n",
    "df_train_pos = df_train_all.loc[positive]\n",
    "df_train_neg = df_train_all.loc[~positive]\n",
    "print('There are ', len(df_train_pos), 'positive samples and ', len(df_train_neg), 'negative samples.')\n",
    "print('Percent positive samples: ', round(len(df_train_pos)*100/len(df_train_all),2),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an imbalance in positive vs negative cases, which would be expected in this dataset. Therefore balancing the data is needed, so that the machine learning model does not always predict negative (no death). Sub-sampling the negative group is one method of doing this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sub-sample negatives so there are an equal number of positive and negative samples.\n",
    "df_train_neg = df_train_neg.sample(n=len(df_train_pos), random_state=42)\n",
    "\n",
    "# merge the positive and negative samples into the final training set\n",
    "df_train_final = pd.concat([df_train_pos, df_train_neg],axis = 0)\n",
    "\n",
    "# shuffle the order of training samples \n",
    "df_train_final = df_train_final.sample(n = len(df_train_final), random_state = 42).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_train_final)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Preprocess text data using Bag of Words model.\n",
    "#### What is Bag of Words?\n",
    "<br>\n",
    "Bag of Words is a method for extracting features from the text for use in machine learning algorithms. Basically, it breaks up a text into individual words, then counts how often each word occurs.\n",
    "\n",
    "A vocabulary will be built using the training dataset. This vocabulary will later be used as a feature in the machine learning model as a basis to predict poitive or negative death."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#viewing an example row of the text column\n",
    "df_train_final.TEXT[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text above shows that it needs some pre-processing (mainly removing the new line command ('\\n'). One way to do this is to create a function to preprocess the text. This way the original data won't be modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(df4):\n",
    "    # This function preprocesses the text by replacing new lines ('\\n')  with a space.\n",
    "    df4.TEXT = df4.TEXT.str.replace('\\n',' ')\n",
    "    return df4\n",
    "# preprocess the text to deal with known issue\n",
    "df_train_final = preprocess_text(df_train_final)\n",
    "df_valid = preprocess_text(df_valid)\n",
    "df_test = preprocess_text(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_final.TEXT[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now import Python's Natural Language Toolkit (NLTK) and other necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "import string # String module provides tools to manipulate strings\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next create a function to separate the text data into tokens (this is called tokenization). Tokens created here will be used to make a vocabulary (set of unique tokens) to be used as a feature for the model. All tokens or top K tokens can be used "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_list = string.digits\n",
    "character_list = '\\n'\n",
    "character_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_tokenizer(text): # create a function that will tokenize the text, and also remove punctuation and numbers\n",
    "    \n",
    "    punc_list = string.punctuation #create list of punctuation marks\n",
    "    number_list=string.digits #create list of numbers\n",
    "    num_punc_list = number_list + punc_list +character_list #combine the lists together\n",
    "    t = str.maketrans(dict.fromkeys(num_punc_list, \" \")) # replace punctuation and numbers with spaces\n",
    "    text = text.lower().translate(t) #lowercase all words\n",
    "    tokens = word_tokenize(text) #tokenize the text \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that tokens can be created from the text, CountVectorizer will turn these tokens into number features to be used in the machine learning model. But first a list of stopwords will be created so that the machine can ignore these words when processing the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "stopwords = set(stopwords.words('english')) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "# fit our vectorizer. \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer(max_features = 3000, tokenizer = text_tokenizer, stop_words = stopwords)\n",
    "vect.fit(df_train_final.TEXT.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vect.vocabulary_)\n",
    "print(type(vect))\n",
    "print(vect.fixed_vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_final.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get an idea of the frequency of words for positive vs negative mortality. This section was based on code from https://towardsdatascience.com/another-twitter-sentiment-analysis-with-python-part-2-333514854913"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Next visualize find the most frequent words.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "neg_doc_matrix = vect.transform(df_train_final[df_train_final.HOSPITAL_EXPIRE_FLAG == 0].TEXT)\n",
    "pos_doc_matrix = vect.transform(df_train_final[df_train_final.HOSPITAL_EXPIRE_FLAG == 1].TEXT)\n",
    "neg_tf = np.sum(neg_doc_matrix,axis=0)\n",
    "pos_tf = np.sum(pos_doc_matrix,axis=0)\n",
    "neg = np.squeeze(np.asarray(neg_tf))\n",
    "pos = np.squeeze(np.asarray(pos_tf))\n",
    "\n",
    "term_freq_df = pd.DataFrame([neg,pos],columns=vect.get_feature_names()).transpose()\n",
    "term_freq_df.columns = ['negative', 'positive']\n",
    "term_freq_df['total'] = term_freq_df['negative'] + term_freq_df['positive']\n",
    "term_freq_df.sort_values(by='total', ascending=False).iloc[:20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a series from the sparse matrix\n",
    "d = pd.Series(term_freq_df.total, \n",
    "              index = term_freq_df.index).sort_values(ascending=False)\n",
    "\n",
    "#Visualize the 100 most frequent words in the text\n",
    "ax = d[:50].plot(kind='bar', figsize=(10,6), width=.8, fontsize=14, rot=90,color = 'b')\n",
    "ax.title.set_size(18)\n",
    "plt.title('Most Frequent Words in Caregiver Notes')\n",
    "plt.ylabel('count')\n",
    "plt.show()\n",
    "ax = d[50:100].plot(kind='bar', figsize=(10,6), width=.8, fontsize=14, rot=90,color = 'b')\n",
    "ax.title.set_size(18)\n",
    "plt.ylabel('count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#with np.printoptions(threshold=np.inf):\n",
    " #   print(neg_doc_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_updated = list(stopwords) + ['pt', 'left', 'right', 'name','patient', 'p','c']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the list of stop words as an argument for CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer(max_features = 3000, \n",
    "                       ngram_range = (1,2)\n",
    "                       tokenizer = text_tokenizer, \n",
    "                       stop_words = stopwords_updated)\n",
    "# this will take a while\n",
    "vect.fit(df_train_final.TEXT.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Again visualize find the most frequent words.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "neg_doc_matrix = vect.transform(df_train_final[df_train_final.HOSPITAL_EXPIRE_FLAG == 0].TEXT)\n",
    "pos_doc_matrix = vect.transform(df_train_final[df_train_final.HOSPITAL_EXPIRE_FLAG == 1].TEXT)\n",
    "neg_tf = np.sum(neg_doc_matrix,axis=0)\n",
    "pos_tf = np.sum(pos_doc_matrix,axis=0)\n",
    "neg = np.squeeze(np.asarray(neg_tf))\n",
    "pos = np.squeeze(np.asarray(pos_tf))\n",
    "\n",
    "term_freq_df = pd.DataFrame([neg,pos],columns=vect.get_feature_names()).transpose()\n",
    "term_freq_df.columns = ['negative', 'positive']\n",
    "term_freq_df['total'] = term_freq_df['negative'] + term_freq_df['positive']\n",
    "term_freq_df.sort_values(by='total', ascending=False).iloc[:20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Again Create a series from the sparse matrix\n",
    "d = pd.Series(term_freq_df.total, \n",
    "              index = term_freq_df.index).sort_values(ascending=False)\n",
    "\n",
    "#Visualize the 100 most frequent words in the text\n",
    "ax = d[:50].plot(kind='bar', figsize=(10,6), width=.8, fontsize=14, rot=90,color = 'b')\n",
    "ax.title.set_size(18)\n",
    "plt.title('Most Frequent Words in Caregiver Notes')\n",
    "plt.ylabel('count')\n",
    "plt.show()\n",
    "ax = d[50:100].plot(kind='bar', figsize=(10,6), width=.8, fontsize=14, rot=90,color = 'b')\n",
    "ax.title.set_size(18)\n",
    "plt.ylabel('count')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the text samples into vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vect = vect.transform(df_train_final.TEXT.values)\n",
    "X_valid_vect = vect.transform(df_valid.TEXT.values)\n",
    "X_test_vect = vect.transform(df_test.TEXT.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vect.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get labels (target variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df_train_final.HOSPITAL_EXPIRE_FLAG\n",
    "y_valid = df_valid.HOSPITAL_EXPIRE_FLAG\n",
    "y_test = df_test.HOSPITAL_EXPIRE_FLAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Build a simple predictive model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf=LogisticRegression(random_state = 42, solver='lbfgs')\n",
    "clf.fit(X_train_vect, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import other models for evaluation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42, n_estimators=100)\n",
    "svm = SVC(random_state=42, gamma='auto', probability=True)\n",
    "gnb = GaussianNB()\n",
    "\n",
    "\n",
    "rf.fit(X_train_vect, y_train)\n",
    "\n",
    "svm.fit(X_train_vect, y_train)\n",
    "\n",
    "gnb.fit(X_train_vect.toarray(), y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate probability of death."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = clf\n",
    "y_train_preds = model.predict_proba(X_train_vect)[:,1]\n",
    "y_valid_preds = model.predict_proba(X_valid_vect)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print mortality predictions for the first 10 samples in the training set\n",
    "print(y_train[:10].values)\n",
    "print(y_train_preds[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gnb\n",
    "y_train_preds_gnb = model.predict_proba(X_train_vect.toarray())[:,1]\n",
    "y_valid_preds_gnb = model.predict_proba(X_valid_vect.toarray())[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=rf\n",
    "y_train_preds_rf = model.predict_proba(X_train_vect)[:,1]\n",
    "y_valid_preds_rf = model.predict_proba(X_valid_vect.toarray())[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = svm\n",
    "y_train_preds_svm = model.predict_proba(X_train_vect)[:,1]\n",
    "y_valid_preds_svm = model.predict_proba(X_valid_vect.toarray())[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print mortality predictions for the first 10 samples in the training set\n",
    "print(y_train[:10].values)\n",
    "print(y_train_preds[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create functions to calculate performanece metrics\n",
    "def calc_accuracy(y_actual, y_pred, thresh):\n",
    "    # this function calculates the accuracy with probability threshold at thresh\n",
    "    return (sum((y_pred > thresh) & (y_actual == 1))+sum((y_pred < thresh) & (y_actual == 0))) /len(y_actual)\n",
    "\n",
    "def calc_recall(y_actual, y_pred, thresh):\n",
    "    # calculates the recall\n",
    "    return sum((y_pred > thresh) & (y_actual == 1)) /sum(y_actual)\n",
    "\n",
    "def calc_precision(y_actual, y_pred, thresh):\n",
    "    # calculates the precision\n",
    "    return sum((y_pred > thresh) & (y_actual == 1)) /sum(y_pred > thresh)\n",
    "\n",
    "def calc_specificity(y_actual, y_pred, thresh):\n",
    "    # calculates specificity\n",
    "    return sum((y_pred < thresh) & (y_actual == 0)) /sum(y_actual ==0)\n",
    "\n",
    "def calc_prevalence(y_actual):\n",
    "    # calculates prevalence\n",
    "    return sum((y_actual == 1)) /len(y_actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOGISTIC REGRESSION\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "fpr_train, tpr_train, thresholds_train = roc_curve(y_train, y_train_preds)\n",
    "fpr_valid, tpr_valid, thresholds_valid = roc_curve(y_valid, y_valid_preds)\n",
    "\n",
    "\n",
    "thresh = 0.5\n",
    "\n",
    "auc_train = roc_auc_score(y_train, y_train_preds)\n",
    "auc_valid = roc_auc_score(y_valid, y_valid_preds)\n",
    "\n",
    "print(\"\\nLogistic Regression Performance Metrics\\n\")\n",
    "print('AUC:')\n",
    "print('Train:%.3f'%auc_train)\n",
    "print('Valid:%.3f'%auc_valid)\n",
    "\n",
    "print('\\nAccuracy:')\n",
    "print('Train:%.3f'%calc_accuracy(y_train, y_train_preds, thresh))\n",
    "print('Valid:%.3f'%calc_accuracy(y_valid, y_valid_preds, thresh))\n",
    "\n",
    "print('\\nRecall:')\n",
    "print('Train:%.3f'%calc_recall(y_train, y_train_preds, thresh))\n",
    "print('Valid:%.3f'%calc_recall(y_valid, y_valid_preds, thresh))\n",
    "\n",
    "print('\\nPrecision:')\n",
    "print('Train:%.3f'%calc_precision(y_train, y_train_preds, thresh))\n",
    "print('Valid:%.3f'%calc_precision(y_valid, y_valid_preds, thresh))\n",
    "\n",
    "print('\\nSpecificity')\n",
    "print('Train:%.3f'%calc_specificity(y_train, y_train_preds, thresh))\n",
    "print('Valid:%.3f'%calc_specificity(y_valid, y_valid_preds, thresh))\n",
    "\n",
    "print('\\nPrevalence')\n",
    "print('Train:%.3f'%calc_prevalence(y_train))\n",
    "print('Valid:%.3f'%calc_prevalence(y_valid))\n",
    "\n",
    "\n",
    "plt.plot(fpr_train, tpr_train,'r-', label = 'Train AUC: %.2f'%auc_train)\n",
    "plt.plot(fpr_valid, tpr_valid,'b-',label = 'Valid AUC: %.2f'%auc_valid)\n",
    "plt.plot([0,1],[0,1],'-k')\n",
    "plt.title('Logistic Regression AUC-ROC')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#GNB\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "fpr_train, tpr_train, thresholds_train = roc_curve(y_train, y_train_preds_gnb)\n",
    "fpr_valid, tpr_valid, thresholds_valid = roc_curve(y_valid, y_valid_preds_gnb)\n",
    "\n",
    "\n",
    "thresh = 0.5\n",
    "\n",
    "auc_train = roc_auc_score(y_train, y_train_preds_gnb)\n",
    "auc_valid = roc_auc_score(y_valid, y_valid_preds_gnb)\n",
    "print(\"\\nNaive Bayes Performance Metrics\\n\")\n",
    "print('Train AUC:%.3f'%auc_train)\n",
    "print('Valid AUC:%.3f'%auc_valid)\n",
    "\n",
    "print('Train accuracy:%.3f'%calc_accuracy(y_train, y_train_preds_gnb, thresh))\n",
    "print('Valid accuracy:%.3f'%calc_accuracy(y_valid, y_valid_preds_gnb, thresh))\n",
    "\n",
    "\n",
    "print('Train recall:%.3f'%calc_recall(y_train, y_train_preds_gnb, thresh))\n",
    "print('Valid recall:%.3f'%calc_recall(y_valid, y_valid_preds_gnb, thresh))\n",
    "\n",
    "print('Train precision:%.3f'%calc_precision(y_train, y_train_preds_gnb, thresh))\n",
    "print('Valid precision:%.3f'%calc_precision(y_valid, y_valid_preds_gnb, thresh))\n",
    "\n",
    "print('Train specificity:%.3f'%calc_specificity(y_train, y_train_preds_gnb, thresh))\n",
    "print('Valid specificity:%.3f'%calc_specificity(y_valid, y_valid_preds_gnb, thresh))\n",
    "\n",
    "print('Train prevalence:%.3f'%calc_prevalence(y_train))\n",
    "print('Valid prevalence:%.3f'%calc_prevalence(y_valid))\n",
    "\n",
    "\n",
    "plt.plot(fpr_train, tpr_train,'r-', label = 'Train AUC: %.2f'%auc_train)\n",
    "plt.plot(fpr_valid, tpr_valid,'b-',label = 'Valid AUC: %.2f'%auc_valid)\n",
    "plt.plot([0,1],[0,1],'-k')\n",
    "plt.title('Naive Bayes AUC-ROC')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#RF\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "fpr_train, tpr_train, thresholds_train = roc_curve(y_train, y_train_preds_rf)\n",
    "fpr_valid, tpr_valid, thresholds_valid = roc_curve(y_valid, y_valid_preds_rf)\n",
    "\n",
    "\n",
    "thresh = 0.5\n",
    "\n",
    "auc_train = roc_auc_score(y_train, y_train_preds_rf)\n",
    "auc_valid = roc_auc_score(y_valid, y_valid_preds_rf)\n",
    "print(\"\\nRandom Forest Performance Metrics\\n\")\n",
    "print('Train AUC:%.3f'%auc_train)\n",
    "print('Valid AUC:%.3f'%auc_valid)\n",
    "\n",
    "print('Train accuracy:%.3f'%calc_accuracy(y_train, y_train_preds_rf, thresh))\n",
    "print('Valid accuracy:%.3f'%calc_accuracy(y_valid, y_valid_preds_rf, thresh))\n",
    "\n",
    "\n",
    "print('Train recall:%.3f'%calc_recall(y_train, y_train_preds_rf, thresh))\n",
    "print('Valid recall:%.3f'%calc_recall(y_valid, y_valid_preds_rf, thresh))\n",
    "\n",
    "print('Train precision:%.3f'%calc_precision(y_train, y_train_preds_rf, thresh))\n",
    "print('Valid precision:%.3f'%calc_precision(y_valid, y_valid_preds_rf, thresh))\n",
    "\n",
    "print('Train specificity:%.3f'%calc_specificity(y_train, y_train_preds_rf, thresh))\n",
    "print('Valid specificity:%.3f'%calc_specificity(y_valid, y_valid_preds_rf, thresh))\n",
    "\n",
    "print('Train prevalence:%.3f'%calc_prevalence(y_train))\n",
    "print('Valid prevalence:%.3f'%calc_prevalence(y_valid))\n",
    "\n",
    "\n",
    "plt.plot(fpr_train, tpr_train,'r-', label = 'Train AUC: %.2f'%auc_train)\n",
    "plt.plot(fpr_valid, tpr_valid,'b-',label = 'Valid AUC: %.2f'%auc_valid)\n",
    "plt.plot([0,1],[0,1],'-k')\n",
    "plt.title('Random Forest AUC-ROC')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#svm\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "fpr_train, tpr_train, thresholds_train = roc_curve(y_train, y_train_preds_svm)\n",
    "fpr_valid, tpr_valid, thresholds_valid = roc_curve(y_valid, y_valid_preds_svm)\n",
    "\n",
    "\n",
    "thresh = 0.5\n",
    "\n",
    "auc_train = roc_auc_score(y_train, y_train_preds_svm)\n",
    "auc_valid = roc_auc_score(y_valid, y_valid_preds_svm)\n",
    "print(\"\\nSVM Performance Metrics\\n\")\n",
    "print('Train AUC:%.3f'%auc_train)\n",
    "print('Valid AUC:%.3f'%auc_valid)\n",
    "\n",
    "print('Train accuracy:%.3f'%calc_accuracy(y_train, y_train_preds_svm, thresh))\n",
    "print('Valid accuracy:%.3f'%calc_accuracy(y_valid, y_valid_preds_svm, thresh))\n",
    "\n",
    "\n",
    "print('Train recall:%.3f'%calc_recall(y_train, y_train_preds_svm, thresh))\n",
    "print('Valid recall:%.3f'%calc_recall(y_valid, y_valid_preds_svm, thresh))\n",
    "\n",
    "print('Train precision:%.3f'%calc_precision(y_train, y_train_preds_svm, thresh))\n",
    "print('Valid precision:%.3f'%calc_precision(y_valid, y_valid_preds_svm, thresh))\n",
    "\n",
    "print('Train specificity:%.3f'%calc_specificity(y_train, y_train_preds_svm, thresh))\n",
    "print('Valid specificity:%.3f'%calc_specificity(y_valid, y_valid_preds_svm, thresh))\n",
    "\n",
    "print('Train prevalence:%.3f'%calc_prevalence(y_train))\n",
    "print('Valid prevalence:%.3f'%calc_prevalence(y_valid))\n",
    "\n",
    "\n",
    "plt.plot(fpr_train, tpr_train,'r-', label = 'Train AUC: %.2f'%auc_train)\n",
    "plt.plot(fpr_valid, tpr_valid,'b-',label = 'Valid AUC: %.2f'%auc_valid)\n",
    "plt.plot([0,1],[0,1],'-k')\n",
    "plt.title('SVM AUC-ROC')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since the performance metrics results and AUC-ROC fared better with Logistic Regression, that will be the model of choice for the remainder of this project.\n",
    "#### The remainder of this analysis will focus on feature engineering and hyperparameter optimization in order to create a stronger algorithm to predict mortality. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First find the features that the classifier is using to make decisions. The features with the highest coefficients predict death and the lowest coefficents predict no death.\n",
    "\n",
    "def get_most_important_features(vectorizer, model, n=5):\n",
    "    index_to_word = {v:k for k,v in vectorizer.vocabulary_.items()}\n",
    "    \n",
    "    # loop for each class\n",
    "    classes ={}\n",
    "    for class_index in range(model.coef_.shape[0]):\n",
    "        word_importances = [(el, index_to_word[i]) for i,el in enumerate(model.coef_[class_index])]\n",
    "        sorted_coeff = sorted(word_importances, key = lambda x : x[0], reverse=True)\n",
    "        tops = sorted(sorted_coeff[:n], key = lambda x : x[0])\n",
    "        bottom = sorted_coeff[-n:]\n",
    "        classes[class_index] = {\n",
    "            'tops':tops,\n",
    "            'bottom':bottom\n",
    "        }\n",
    "    return classes\n",
    "\n",
    "importance = get_most_important_features(vect, clf, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next, plot the most important features\n",
    "\n",
    "def plot_important_words(top_scores, top_words, bottom_scores, bottom_words, name):\n",
    "    y_pos = np.arange(len(top_words))\n",
    "    top_pairs = [(a,b) for a,b in zip(top_words, top_scores)]\n",
    "    top_pairs = sorted(top_pairs, key=lambda x: x[1])\n",
    "    \n",
    "    bottom_pairs = [(a,b) for a,b in zip(bottom_words, bottom_scores)]\n",
    "    bottom_pairs = sorted(bottom_pairs, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    top_words = [a[0] for a in top_pairs]\n",
    "    top_scores = [a[1] for a in top_pairs]\n",
    "    \n",
    "    bottom_words = [a[0] for a in bottom_pairs]\n",
    "    bottom_scores = [a[1] for a in bottom_pairs]\n",
    "    \n",
    "    fig = plt.figure(figsize=(10, 15))  \n",
    "\n",
    "    plt.subplot(121)\n",
    "    plt.barh(y_pos,bottom_scores, align='center', alpha=0.5)\n",
    "    plt.title('Negative', fontsize=20)\n",
    "    plt.yticks(y_pos, bottom_words, fontsize=14)\n",
    "    plt.suptitle('Key words', fontsize=16)\n",
    "    plt.xlabel('Importance', fontsize=20)\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    plt.barh(y_pos,top_scores, align='center', alpha=0.5)\n",
    "    plt.title('Positive', fontsize=20)\n",
    "    plt.yticks(y_pos, top_words, fontsize=14)\n",
    "    plt.suptitle(name, fontsize=16)\n",
    "    plt.xlabel('Importance', fontsize=20)\n",
    "    \n",
    "    plt.subplots_adjust(wspace=0.8)\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "  \n",
    "top_scores = [a[0] for a in importance[0]['tops']]\n",
    "top_words = [a[1] for a in importance[0]['tops']]\n",
    "bottom_scores = [a[0] for a in importance[0]['bottom']]\n",
    "bottom_words = [a[1] for a in importance[0]['bottom']]\n",
    "\n",
    "plot_important_words(top_scores, top_words, bottom_scores, bottom_words, \"Most important words\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Begin by plotting a learning curve. This will help to assure that we have enough samples to make good predictions.  \n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    \"\"\"\n",
    "    Generate a simple plot of the test and training learning curve.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
    "        An object of that type which is cloned for each validation.\n",
    "\n",
    "    title : string\n",
    "        Title for the chart.\n",
    "\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Training vector, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "\n",
    "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
    "        Target relative to X for classification or regression;\n",
    "        None for unsupervised learning.\n",
    "\n",
    "    ylim : tuple, shape (ymin, ymax), optional\n",
    "        Defines minimum and maximum yvalues plotted.\n",
    "\n",
    "    cv : int, cross-validation generator or an iterable, optional\n",
    "        Determines the cross-validation splitting strategy.\n",
    "        Possible inputs for cv are:\n",
    "          - None, to use the default 3-fold cross-validation,\n",
    "          - integer, to specify the number of folds.\n",
    "          - An object to be used as a cross-validation generator.\n",
    "          - An iterable yielding train/test splits.\n",
    "\n",
    "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
    "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
    "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
    "\n",
    "        Refer :ref:`User Guide <cross_validation>` for the various\n",
    "        cross-validators that can be used here.\n",
    "\n",
    "    n_jobs : integer, optional\n",
    "        Number of jobs to run in parallel (default 1).\n",
    "        \"\"\"\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Number of Training examples\")\n",
    "    plt.ylabel(\"AUC\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, scoring = 'roc_auc')\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"b\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"b\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "title = \"Learning Curves (Logistic Regression)\"\n",
    "# Cross validation with 5 iterations to get smoother mean test and train\n",
    "# score curves, each time with 20% data randomly selected as a validation set.\n",
    "cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
    "estimator = LogisticRegression( C = 0.0001, penalty = 'l2')#\n",
    "plot_learning_curve(estimator, title, X_train_vect, y_train, ylim=(0.2, 1.01), cv=cv, n_jobs=4)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AUC of the training set is high, and the CV score is also high, and begins to approach the Training score curve as number of samples increases. This means that the  model is learning well with the number of samples provided; more data would not necessarily make the model better. Some overfitting is indicated since there is a gap between the training score and cross validation score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(clf.coef_.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at the most important words for the model.\n",
    "\n",
    "feature_importances = pd.DataFrame(clf.coef_[0],\n",
    "                                   index = vect.get_feature_names(),\n",
    "                                    columns=['importance']).sort_values('importance',\n",
    "                                                                        ascending=False)\n",
    "\n",
    "num=25\n",
    "ylocs = np.arange(num)\n",
    "# get the feature importance for top num and sort in reverse order\n",
    "values_to_plot = feature_importances.iloc[:num].values.ravel()[::-1]\n",
    "feature_labels = list(feature_importances.iloc[:num].index)[::-1]\n",
    "\n",
    "plt.figure(num=None, figsize=(8, 15), dpi=80, facecolor='w', edgecolor='k');\n",
    "plt.barh(ylocs, values_to_plot, align = 'center')\n",
    "plt.ylabel('Features')\n",
    "plt.xlabel('Importance Score')\n",
    "plt.title('Positive Feature Importance Score - Logistic Regression')\n",
    "plt.yticks(ylocs, feature_labels)\n",
    "plt.show()\n",
    "\n",
    "values_to_plot = feature_importances.iloc[-num:].values.ravel()\n",
    "feature_labels = list(feature_importances.iloc[-num:].index)\n",
    "\n",
    "plt.figure(num=None, figsize=(8, 15), dpi=80, facecolor='w', edgecolor='k');\n",
    "plt.barh(ylocs, values_to_plot, align = 'center')\n",
    "plt.ylabel('Features')\n",
    "plt.xlabel('Importance Score')\n",
    "plt.title('Negative Feature Importance Score - Logistic Regression')\n",
    "plt.yticks(ylocs, feature_labels)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find best value of C\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "Cs = [0.00001, 0.0001, 0.001, 0.005, 0.01, 0.05, 0.1]\n",
    "train_aucs = np.zeros(len(Cs))\n",
    "valid_aucs = np.zeros(len(Cs))\n",
    "\n",
    "for ii in range(len(Cs)):\n",
    "    C = Cs[ii]\n",
    "    print('\\n C:', C)\n",
    "    \n",
    "    # logistic regression\n",
    "    \n",
    "    clf=LogisticRegression(C = C, penalty = 'l2', random_state = 42)\n",
    "    clf.fit(X_train_vect, y_train)\n",
    "\n",
    "    model = clf\n",
    "    y_train_preds = model.predict_proba(X_train_vect)[:,1]\n",
    "    y_valid_preds = model.predict_proba(X_valid_vect)[:,1]\n",
    "\n",
    "    auc_train = roc_auc_score(y_train, y_train_preds)\n",
    "    auc_valid = roc_auc_score(y_valid, y_valid_preds)\n",
    "    print('Train AUC:%.3f'%auc_train)\n",
    "    print('Valid AUC:%.3f'%auc_valid)\n",
    "    train_aucs[ii] = auc_train\n",
    "    valid_aucs[ii] = auc_valid\n",
    "\n",
    "\n",
    "plt.plot(Cs, train_aucs,'bo-', label ='Train')\n",
    "plt.plot(Cs, valid_aucs, 'ro-', label='Valid')\n",
    "plt.legend()\n",
    "plt.xlabel('Logistic Regression - C')\n",
    "plt.ylabel('AUC')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As C gets larger, overfitting quickly occurs as shown by the gap between training and validation. The best values for C are 0.0001 and 0.001, based on the individual ROC-AUC and difference between  training and validation ROC-AUC scores at those values of C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Countvectorizer is currently using 3000 maximum tokens. Next examine the effect of Max_features on performance.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "num_features = [100,300,1000,3000,10000,30000]\n",
    "train_aucs = np.zeros(len(num_features))\n",
    "valid_aucs = np.zeros(len(num_features))\n",
    "\n",
    "for ii in range(len(num_features)):\n",
    "    num = num_features[ii]\n",
    "    print('\\nnumber of features:', num)\n",
    "    vect = CountVectorizer(lowercase = True, max_features = num, \n",
    "                           tokenizer = text_tokenizer,stop_words =stopwords_updated)\n",
    "\n",
    "    # This could take a while\n",
    "    vect.fit(df_train_final.TEXT.values)\n",
    "\n",
    "    X_train_vect = vect.transform(df_train_final.TEXT.values)\n",
    "    X_valid_vect = vect.transform(df_valid.TEXT.values)\n",
    "    y_train = df_train_final.HOSPITAL_EXPIRE_FLAG\n",
    "    y_valid = df_valid.HOSPITAL_EXPIRE_FLAG\n",
    "    \n",
    "    clf=LogisticRegression(C = 0.0001, penalty = 'l2', random_state = 42)\n",
    "    clf.fit(X_train_vect, y_train)\n",
    "\n",
    "    model = clf\n",
    "    y_train_preds = model.predict_proba(X_train_vect)[:,1]\n",
    "    y_valid_preds = model.predict_proba(X_valid_vect)[:,1]\n",
    "\n",
    "    auc_train = roc_auc_score(y_train, y_train_preds)\n",
    "    auc_valid = roc_auc_score(y_valid, y_valid_preds)\n",
    "    print('Train AUC: %.3f'%auc_train)\n",
    "    print('Valid AUC:%.3f'%auc_valid)\n",
    "    train_aucs[ii] = auc_train\n",
    "    valid_aucs[ii] = auc_valid\n",
    "\n",
    "plt.plot(num_features, train_aucs,'bo-', label ='Train')\n",
    "plt.plot(num_features, valid_aucs, 'ro-', label='Valid')\n",
    "plt.legend()\n",
    "plt.xlabel('Number of features')\n",
    "plt.ylabel('AUC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As max_features gets larger, some overfitting quickly occurs. AUC begins leveling out for both Training and Validation at about 3000-4000 words. Therefore, the current setting for max_features is appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "\n",
    "clf_param_grid = {'penalty': ['l1','l2'],\n",
    "                  'C': [0.001, 0.01, 0.1, 1, 2],\n",
    "                  'class_weight': [{1:0.5, 0:0.5}, {1:0.6, 0:0.4}, {1:0.7, 0:0.3}, {1:0.8, 0:0.2}, {1:0.9, 0:0.1}]\n",
    "                   }\n",
    "clf = GridSearchCV(\n",
    "    estimator=clf,\n",
    "    param_grid=clf_param_grid,               \n",
    "    n_jobs=-1,\n",
    "    scoring='f1',\n",
    "    cv=StratifiedKFold(n_splits=5,shuffle=True))\n",
    "logistic_best_model = clf.fit(X_train_vect, y_train)\n",
    "\n",
    "print('Best Logistic Score (F1): ', logistic_best_model.best_score_)\n",
    "print('Best Logistic Parameters: ', logistic_best_model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix \n",
    "\n",
    "#Re-running predictions and see classification report on this grid object\n",
    "clf_best_model = logistic_best_model.predict(X_test_vect) \n",
    "\n",
    "print('Logistic Classification Report\\n\\n', classification_report(y_test, clf_best_model))\n",
    "print('Confusion Matrix \\n\\n', confusion_matrix(y_test, clf_best_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf_best=LogisticRegression(C = 0.001, penalty = 'l2',class_weight={1: 0.6, 0: 0.4}, random_state = 42, n_jobs=-1)\n",
    "clf_best.fit(X_train_vect, y_train)\n",
    "\n",
    "model = clf_best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_preds = model.predict_proba(X_train_vect)[:,1]\n",
    "y_valid_preds = model.predict_proba(X_valid_vect)[:,1]\n",
    "y_test_preds = model.predict_proba(X_test_vect)[:,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fpr_train, tpr_train, thresholds_train = roc_curve(y_train, y_train_preds)\n",
    "fpr_valid, tpr_valid, thresholds_valid = roc_curve(y_valid, y_valid_preds)\n",
    "fpr_test, tpr_test, thresholds_test = roc_curve(y_test, y_test_preds)\n",
    "\n",
    "\n",
    "thresh = 0.5\n",
    "\n",
    "auc_train = roc_auc_score(y_train, y_train_preds)\n",
    "auc_valid = roc_auc_score(y_valid, y_valid_preds)\n",
    "auc_test = roc_auc_score(y_test, y_test_preds)\n",
    "\n",
    "print(\"\\nBest Model Performance Metrics\\n\")\n",
    "print('\\nAUC:')\n",
    "print('Train:%.3f'%auc_train)\n",
    "print('Valid:%.3f'%auc_valid)\n",
    "print('Test:%.3f'%auc_test)\n",
    "\n",
    "print('\\nAccuracy')\n",
    "print('Train:%.3f'%calc_accuracy(y_train, y_train_preds, thresh))\n",
    "print('Valid:%.3f'%calc_accuracy(y_valid, y_valid_preds, thresh))\n",
    "print('Test:%.3f'%calc_accuracy(y_test, y_test_preds, thresh))\n",
    "\n",
    "print('\\nRecall')\n",
    "print('Train:%.3f'%calc_recall(y_train, y_train_preds, thresh))\n",
    "print('Valid:%.3f'%calc_recall(y_valid, y_valid_preds, thresh))\n",
    "print('Test:%.3f'%calc_recall(y_test, y_test_preds, thresh))\n",
    "\n",
    "print('\\nPrecision')\n",
    "print('Train:%.3f'%calc_precision(y_train, y_train_preds, thresh))\n",
    "print('Valid:%.3f'%calc_precision(y_valid, y_valid_preds, thresh))\n",
    "print('Test:%.3f'%calc_precision(y_test, y_test_preds, thresh))\n",
    "\n",
    "print('\\nSpecificity')\n",
    "print('Train:%.3f'%calc_specificity(y_train, y_train_preds, thresh))\n",
    "print('Valid:%.3f'%calc_specificity(y_valid, y_valid_preds, thresh))\n",
    "print('Test:%.3f'%calc_specificity(y_test, y_test_preds, thresh))\n",
    "\n",
    "print('\\nPrevalence')\n",
    "print('Train:%.3f'%calc_prevalence(y_train))\n",
    "print('Valid:%.3f'%calc_prevalence(y_valid))\n",
    "print('Test:%.3f'%calc_prevalence(y_test))\n",
    "\n",
    "\n",
    "plt.plot(fpr_train, tpr_train,'r-', label = 'Train AUC: %.2f'%auc_train)\n",
    "plt.plot(fpr_valid, tpr_valid,'b-',label = 'Valid AUC: %.2f'%auc_valid)\n",
    "plt.plot(fpr_test, tpr_test,'g-',label = 'Test AUC: %.2f'%auc_test)\n",
    "\n",
    "plt.plot([0,1],[0,1],'-k')\n",
    "plt.title('Best Model AUC-ROC')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix \n",
    "\n",
    "#Re-running predictions and see classification report on this grid object\n",
    "clf_best_model = model.predict(X_test_vect) \n",
    "\n",
    "print('Logistic Classification Report\\n\\n', classification_report(y_test, clf_best_model, labels=[1,0]))\n",
    "print('Confusion Matrix \\n\\n', confusion_matrix(y_test, clf_best_model, labels=[1,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# save the model for later use\n",
    "filename = 'model.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some time later...\n",
    " \n",
    "# load the model from disk\n",
    "loaded_model = pickle.load(open('model.sav', 'rb'))\n",
    "result = model.score(X_test_vect, y_test)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
